{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1de156",
   "metadata": {},
   "source": [
    "Transformer models handles text by converting them into numbers. i.e. it converts text -> tokens -> numbers \n",
    "for this we use AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "994b71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ea429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asim/Desktop/HF LLM/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f3cb8c37e54adf8f8ca280a956e9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b96c65fa28a481a8b87679a856c9e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612e09862c194c35a7033f15ab1b1573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189c804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Hello, I ' m a single sentence! [SEP]\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86de850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1327, 112, 188, 1146, 136, 102, 1192, 1833, 1363, 1137, 1184, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"What's up?\", \"You doing good or what?\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f34da",
   "metadata": {},
   "source": [
    "Lets take a look at tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391bd826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1327,  112,  188, 1146,  136,  102, 1192, 1833, 1363, 1137, 1184,\n",
      "          136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"What's up?\", \"You doing good or what?\", return_tensors = \"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c39f6",
   "metadata": {},
   "source": [
    "Now lets do the padding so that model can normalize i.e. make the sentences of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d67e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1327,  112,  188, 1146,  136,  102, 1192, 1833, 1363, 1137, 1184,\n",
      "          136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input1 = tokenizer([\"What's up?\", \"You doing good or what?\"], padding = True,  return_tensors = \"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c6ed8",
   "metadata": {},
   "source": [
    "Lets truncate the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2056d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\n",
    "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
    "    truncation=True,\n",
    ")\n",
    "print(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139907f",
   "metadata": {},
   "source": [
    "padding + truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "698ada4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  102],\n",
      "        [ 101,  146, 1821, 1363,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_in = tokenizer(\n",
    "    [\"How are you?\", \"I am good\"],\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 5,\n",
    ")\n",
    "\n",
    "print(encoded_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db091397",
   "metadata": {},
   "source": [
    "Batch size = number of items inside the list []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd300847",
   "metadata": {},
   "source": [
    "ADDING SPECIAL TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf49432",
   "metadata": {},
   "source": [
    "beginning of a sentence ([CLS]) \n",
    "\n",
    "\n",
    "separator between sentences ([SEP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104f0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8790, 1184, 112, 188, 1146, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[CLS] Hi what ' s up? [SEP]\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = tokenizer(\"Hi what's up?\")\n",
    "print(en)\n",
    "tokenizer.decode(en[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42406846",
   "metadata": {},
   "source": [
    "These special tokens are automatically added by the tokenizer. Not all models need special tokens; they are primarily used when a model was pretrained with them, in which case the tokenizer will add them since the model expects them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b076d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 14941, 1139, 1299, 119, 102], [101, 1725, 1177, 3021, 136, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "seq = tokenizer([\n",
    "    \"Yo my man.\",\n",
    "    \"why so serious?\"\n",
    "]\n",
    ")\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "542e3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [\n",
    "        101,\n",
    "        1045,\n",
    "        1005,\n",
    "        2310,\n",
    "        2042,\n",
    "        3403,\n",
    "        2005,\n",
    "        1037,\n",
    "        17662,\n",
    "        12172,\n",
    "        2607,\n",
    "        2026,\n",
    "        2878,\n",
    "        2166,\n",
    "        1012,\n",
    "        102,\n",
    "    ],\n",
    "    [101, 1045, 5223, 2023, 2061, 2172, 999, 102],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15daa530",
   "metadata": {},
   "source": [
    "As this \"array\" is already of rectangular shape, lets convert it to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0b07836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "batch = tokenizer.pad(\n",
    "    {\"input_ids\": encoded_sequences},\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efb3f077",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutputs\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "028d42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():   # inference mode\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5b017c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3412, -0.1088,  0.1147,  ..., -0.2650,  0.1881, -0.1104],\n",
      "         [-0.0316, -0.9939,  0.3383,  ..., -0.2290,  0.5234,  0.2093],\n",
      "         [ 0.1651, -0.8795,  0.5937,  ..., -0.0165, -0.0986,  0.1480],\n",
      "         ...,\n",
      "         [-0.1512, -0.5010,  0.1329,  ..., -0.3520, -0.1183,  0.2425],\n",
      "         [ 0.1559, -0.4795,  0.1415,  ..., -0.4274, -0.2023,  0.2730],\n",
      "         [ 0.9356, -0.5068,  0.2157,  ..., -0.9903,  0.0920, -0.6285]],\n",
      "\n",
      "        [[-0.0725,  0.0540, -0.0037,  ...,  0.1450,  0.2381, -0.0164],\n",
      "         [-0.1619, -0.3062, -0.2282,  ...,  0.3782, -0.1170,  0.1295],\n",
      "         [-0.1173, -0.1003,  0.1703,  ...,  0.3227, -0.1996,  0.1646],\n",
      "         ...,\n",
      "         [-0.1968, -0.3095, -0.2132,  ...,  0.3795, -0.0715,  0.0696],\n",
      "         [-0.1486, -0.2172, -0.1510,  ...,  0.3222,  0.1034, -0.0320],\n",
      "         [-0.1927, -0.1105,  0.1996,  ...,  0.4188,  0.2443, -0.0229]]]), pooler_output=tensor([[-0.7012,  0.5009,  0.9999,  ...,  1.0000, -0.6453,  0.9923],\n",
      "        [-0.7491,  0.3085,  0.9996,  ...,  0.9998, -0.8474,  0.9730]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3574ae6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sentence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(tokens)\n\u001b[1;32m      3\u001b[0m sentence\n",
      "File \u001b[0;32m~/Desktop/HF LLM/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    426\u001b[0m ids_to_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids) \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 428\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids_to_skip:\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "446b8822",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mbatch_tokens\u001b[49m):\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(tokens)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "for i, tokens in enumerate(batch_tokens):\n",
    "    text = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"Sentence {i}:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "198d5d7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HF LLM/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    426\u001b[0m ids_to_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids) \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 428\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids_to_skip:\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "batch_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fad96580",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokens = [\n",
    "    tokenizer.convert_ids_to_tokens(sentence_ids)\n",
    "    for sentence_ids in input_ids\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66937672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 0:\n",
      " 101 → [CLS]\n",
      "1045 → 正\n",
      "1005 → 國\n",
      "2310 → themselves\n",
      "2042 → ##ine\n",
      "3403 → search\n",
      "2005 → hours\n",
      "1037 → 月\n",
      "17662 → Riders\n",
      "12172 → stern\n",
      "2607 → changes\n",
      "2026 → largest\n",
      "2878 → silver\n",
      "2166 → previous\n",
      "1012 → 夫\n",
      " 102 → [SEP]\n",
      "\n",
      "Sentence 1:\n",
      " 101 → [CLS]\n",
      "1045 → 正\n",
      "5223 → rapidly\n",
      "2023 → kept\n",
      "2061 → White\n",
      "2172 → vocals\n",
      " 999 → 司\n",
      " 102 → [SEP]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n",
      "   0 → [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i, tokens in enumerate(batch_tokens):\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    for tid, tok in zip(input_ids[i], tokens):\n",
    "        print(f\"{int(tid):>4} → {tok}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cda2b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: [CLS] 正 國 themselvesine search hours 月 Riders stern changes largest silver previous 夫 [SEP]\n",
      "Sentence 1: [CLS] 正 rapidly kept White vocals 司 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i, tokens in enumerate(batch_tokens):\n",
    "    text = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"Sentence {i}:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0ca22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
