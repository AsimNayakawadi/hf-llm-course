{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350fcdd0",
   "metadata": {},
   "source": [
    "Transfer Learning is applied by cutting of the head(i.e. its last layers) of pretrained model while keeping its body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d2973",
   "metadata": {},
   "source": [
    "Pretraining = training a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b1f75",
   "metadata": {},
   "source": [
    "Fine-tuning = training done after a model has been pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13746931",
   "metadata": {},
   "source": [
    "Transformer architecture\n",
    "Encoder \n",
    "Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a6ec3",
   "metadata": {},
   "source": [
    "----------------Encoder----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661aa540",
   "metadata": {},
   "source": [
    "Encoder : This is where the inuput goes. It receives an input and builds a representation of it (its features). i.e. the model is optimized to acquire understanding from the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56417427",
   "metadata": {},
   "source": [
    "Good for tasks that require understanding of the inputs, such as sentence classificaton and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026223ef",
   "metadata": {},
   "source": [
    "While training encoder receives inputs (sentences) in a certain language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14484e9e",
   "metadata": {},
   "source": [
    "attention layer of encoder can use all the words in a sentence. ( so that it can look at what is before and after the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3c723",
   "metadata": {},
   "source": [
    "Self-attention in the encoder is bidirectional:\n",
    "\n",
    "The token at position 3 can “look” at tokens 0,1,2,4,5,… (everything before and after)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f4b04",
   "metadata": {},
   "source": [
    "The encoder sees the whole input sentence at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fe39d",
   "metadata": {},
   "source": [
    "Encoder self-attention: each input token can attend to all other input tokens (both before and after) to build contextual embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a574c28",
   "metadata": {},
   "source": [
    "----------------Decoder----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103db855",
   "metadata": {},
   "source": [
    "Decoder: It takes the input from encoder i.e. encoders representation(features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e731ab0",
   "metadata": {},
   "source": [
    "Decoder works sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c8c5f",
   "metadata": {},
   "source": [
    "the first attention layer in a decoder block pays attention to all (past) inputs to the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f95d3",
   "metadata": {},
   "source": [
    "the second attention layer uses the output of the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f508277",
   "metadata": {},
   "source": [
    "Decoder receives same sentences as encoder but in the desired targetted language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25fbc7",
   "metadata": {},
   "source": [
    "The decoder is autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141086a",
   "metadata": {},
   "source": [
    "Decoder self-attention:\n",
    "\n",
    "Each output word can look only at itself and previous output words (masked, left side only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5255c5",
   "metadata": {},
   "source": [
    "Decoder self-attention (masked): each output token can attend only to itself and previous output tokens (left-to-right, no future)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d9646",
   "metadata": {},
   "source": [
    "Good for generative tasks such as text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddecf1",
   "metadata": {},
   "source": [
    "Decoder cross-attention:\n",
    "\n",
    "Each output word can look at all encoder outputs (full input sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634356a6",
   "metadata": {},
   "source": [
    "Decoder cross-attention: each output token can attend to all encoder outputs, i.e. the entire input sentence, while generating the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73b9e6",
   "metadata": {},
   "source": [
    "Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8790a",
   "metadata": {},
   "source": [
    "Attention Layer : a layer in transformer that gives attention to not just its one word but others in its surrounding too. Just because to understand the context of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba378b3",
   "metadata": {},
   "source": [
    "Architecture: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
    "\n",
    "Checkpoints: These are the weights that will be loaded in a given architecture.\n",
    "\n",
    "Model: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b3769",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf-llm-venv)",
   "language": "python",
   "name": "hf-llm-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
